# PLM — Perception-Language Model (Meta AI)

## Overview

- **Goal:** Combine **text, image, and video** understanding to enable **multimodal reasoning** within a single foundation model.
- **Problem:** Most advanced vision models (e.g., GPT-4V, Gemini, Claude 3 Opus) are **closed-source**.
- **Solution:** Meta introduces **PLM**, an **open-source multimodal foundation model** that achieves detailed visual understanding **without relying on distillation** from proprietary models. Most open source models (e.g., LLaVA, MiniGPT-4) use **distilled captions** from closed models, which limits their capabilities. Essentially they don't learn true vision-language alignment, they learn to mimic the closed model's outputs.

---

## Architecture

### Core Components

| Modality          | Encoder / Processing    | Output                                            |
| ----------------- | ----------------------- | ------------------------------------------------- |
| **Text**          | LLaMA-3 Tokenizer       | Token embeddings                                  |
| **Image / Video** | PE (Perception Encoder) | Visual features                                   |
| **Projection**    | 2-layer MLP             | Maps visual features into LLaMA-3 embedding space |

### High-Level Flow

$$
\text{Image/Video} \xrightarrow{\text{PE}} \mathbf{v} \xrightarrow{\text{MLP}} \mathbf{h}_v
$$

$$
\text{Text} \xrightarrow{\text{Tokenizer}} \mathbf{h}_t
$$

$$
[\mathbf{h}_v, \mathbf{h}_t] \to \text{LLaMA-3} \to \text{Next-Token Prediction}
$$

So, PLM **integrates image/video features and text tokens into the same embedding space**, allowing **multimodal reasoning** inside LLaMA-3.

---

## Vision Module: High-Resolution & Dynamic Tiling

- Supports **high-resolution images** via **dynamic tiling**:
  Large image → split into smaller **tiles** → encoded independently → **average-pooled** features aggregated.

$$
\mathbf{v} = \text{AvgPool}\big({\text{PE}(t_i)}*{i=1}^{N*{\text{tiles}}}\big)
$$

This enables efficient processing of large, detailed scenes (e.g., documents, dense frames) without memory blow-up.

---

## Training Pipeline

PLM training occurs in **three major stages**:

### 1️ Warm-up Phase

- Train **only the projector (2-layer MLP)**.
- Use **synthetic captions** — _not distilled_ from closed models.
- Captions generated from **metadata**, **OCR**, and **LLM-based captioning** using only open tools.
- Purpose: align visual feature space with LLaMA-3 embedding space. The Project is the only model at this point that has not been trained at all.

---

### 2️ Mid-Training Phase

- Expand to **larger multimodal datasets** (images + videos).
- Combine **synthetic captions** and **QA pairs** generated by LLMs.
- Train both **Perception Encoder (PE)** and **Projector** jointly with **LLaMA-3 frozen or partially unfrozen**.
- Objective: build _cross-modal alignment_ and _contextual reasoning_.

---

### 3️ Supervised Fine-Tuning (SFT)

- Use **human-annotated** complex samples.
- Fine-tune for high-level multimodal reasoning tasks:

  - **FGQA (Fine-Grained Question Answering):** reasoning over fine spatial details.
  - **STC (Spatio-Temporal Captioning):** describing motion, relationships, or dynamic scenes.

---

## Objective Function

Mainly **next-token prediction** in a multimodal context:

$$
\mathcal{L}*{\text{PLM}} = -\sum_t \log p*\theta(x_t \mid x_{<t}, \mathbf{h}_v)
$$

where:

- ( x_t ): next token (text, answer, or caption),
- ( \mathbf{h}\_v ): projected visual embedding sequence from PE→MLP,
- ( p\_\theta ): probability distribution predicted by the multimodal LLaMA-3 decoder.

---

## Results & Capabilities

- **Outperforms SOTA** in:

  - **Image Captioning**
  - **Fine-Grained QA (FGQA)**
  - **Video Question Answering**

- Demonstrates **detailed visual grounding** and **multi-turn reasoning** comparable to proprietary multimodal models.
- Provides **open-source transparency** and **reproducible data pipeline**, unlike closed competitors.

---

## Conceptual Context

| Model                | Modality         | Learning Style        | Key Idea                                                                   |
| -------------------- | ---------------- | --------------------- | -------------------------------------------------------------------------- |
| **CLIP (OpenAI)**    | Image-Text       | Contrastive           | Align image & text embeddings via cosine similarity                        |
| **LLaVA / Kosmos-2** | Image-Text       | Distillation          | Train on captions from closed models (GPT-4V)                              |
| **PLM (Meta)**       | Image-Text-Video | Self-supervised + SFT | Learn alignment from synthetic + open data, fine-tuned with human feedback |

PLM can be viewed as a **unified multimodal reasoning model**, not purely contrastive (like CLIP) nor purely generative (like diffusion or autoregressive decoders), but rather **autoregressive across modalities** — using **causal next-token prediction** on concatenated text-visual embeddings.

---

## Key Takeaways

- **PLM = Perception Encoder (PE) + Projector (2-layer MLP) + LLaMA-3 Backbone**
- Learns **joint multimodal representation** without closed-model distillation.
- Handles **high-resolution images & videos** using **dynamic tiling + pooling**.
- Uses **synthetic + human-labeled** data across three training stages.
- Enables **fine-grained**, **spatio-temporal**, and **contextual reasoning**.
- Marks a significant step toward **open multimodal foundation models**.

---
